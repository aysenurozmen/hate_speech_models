{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f08981-e498-46fc-835e-0bd3cdfb9083",
   "metadata": {},
   "source": [
    "## Character-Level CNN-LSTM model from https://doi.org/10.3390/app131911104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbe97993-7f3c-407f-af36-2694e12fe2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aysen\\anaconda3\\envs\\hate_speech_classification\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, GlobalMaxPooling1D, LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from text_preprocess import clean_text\n",
    "from results import plot_results\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7946e18b-6f54-4763-8006-179ede9d540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_one_hot(text):\n",
    "    \n",
    "    # English character set\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz \"\n",
    "    unknown_char = \"UNK\"\n",
    "\n",
    "    # Create character-to-index mapping\n",
    "    char_to_index = {char: i for i, char in enumerate(alphabet, start=1)}  # Start index from 1\n",
    "    char_to_index[unknown_char] = 0  # Index 0 for unknown characters\n",
    "\n",
    "    # Preprocess input text\n",
    "    preprocessed_text = clean_text(text)\n",
    "\n",
    "    # Tokenize the text into character indices\n",
    "    tokenized_text = [char_to_index.get(char.lower(), char_to_index[unknown_char]) for char in preprocessed_text]\n",
    "    \n",
    "    # Zero-pad to fix the length to 350 characters -> Max length of input texts is 349 without preprocess, 326 with preprocess\n",
    "    max_length = 330\n",
    "    padded_text = pad_sequences([tokenized_text], maxlen=max_length, padding='post', truncating='post')[0]\n",
    "\n",
    "    # One-hot encoding\n",
    "    one_hot_encoding = np.eye(len(alphabet) + 1)[padded_text]  # +1 for the unknown character\n",
    "\n",
    "    return one_hot_encoding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d9b644-a197-4a54-a9a0-917c39cb64e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entry with the maximum number of words has index: 1160\n",
      "The maximum number of words is: 326\n",
      "The text of the entry is: the reason y i dropped out of school was because either i drop out or i got kicked out i always fought with the packi gang at my school and i was never going to back down in auto class i handed the n igger a monkey wrench and told him the name suited him aswell all the nonwhites hated me because i wore a nazi punk and a white pride patch on my coat and backpack a got in a fight with some black chick and i won the next day her butchy sister caught up with me in the hall way and said that after school her and i where gonna fight i told her no if u wanna fight we re gonna fight right here right now not after school where u can gather the rest of u monkies up so it s an uneven fight she huffed and stormed away we neevr did get to fight the last straw was the first day of school i had a forgein substitute teacher so i was talkin and and this guy was on my case about turning around and getting my work done he had to ask me this a few times and then finially i stood up and said look i m white ur black i should be telling u whut to do ohhh man my guidance conslour did nt like that nor did my principal this was nt at school this happened a month or so ago but these people went to my school i had a n igger try to rob me at knife point in the forest while his gf sat back and watched my friend came by and jumped him and i personally went after the chick she ran and i caught up to her and smashed her head into a tree thats only some of my stories i got a few more but i dont have the time\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_csv(\"C:\\\\Users\\\\aysen\\\\Documents\\\\GitHub\\\\hate_speech_models\\\\data\\\\annotations_metadata_train.csv\")\n",
    "test_df = pd.read_csv(\"C:\\\\Users\\\\aysen\\\\Documents\\\\GitHub\\\\hate_speech_models\\\\data\\\\annotations_metadata_test.csv\")\n",
    "\n",
    "####################################################################################\n",
    "train_df['preprocessed_text'] = train_df['text'].apply(clean_text)\n",
    "train_df['num_words'] = train_df['preprocessed_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Find the entry with the maximum number of words\n",
    "max_words_entry = train_df['num_words'].idxmax()\n",
    "\n",
    "# Get the corresponding entry from the DataFrame\n",
    "max_words_text = train_df.loc[max_words_entry, 'preprocessed_text']\n",
    "\n",
    "print(f\"The entry with the maximum number of words has index: {max_words_entry}\")\n",
    "print(f\"The maximum number of words is: {train_df['num_words'].max()}\")\n",
    "print(f\"The text of the entry is: {max_words_text}\")\n",
    "####################################################################################\n",
    "\n",
    "# Text preprocessing\n",
    "X_train_one_hot = train_df['text'].apply(text_to_one_hot)\n",
    "X_test_one_hot = test_df['text'].apply(text_to_one_hot)\n",
    "\n",
    "# Binary label encoding\n",
    "y_train = (train_df['label'] == 'hate').astype(int)\n",
    "y_test = (test_df['label'] == 'hate').astype(int)\n",
    "\n",
    "# Convert pandas series to tensorflow tensors\n",
    "X_train_tensor = tf.convert_to_tensor(X_train_one_hot.tolist(), dtype=tf.float32)\n",
    "X_test_tensor = tf.convert_to_tensor(X_test_one_hot.tolist(), dtype=tf.float32)\n",
    "\n",
    "y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b854455-bd62-4a6d-be1a-4d2dd09c618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "patience = 10\n",
    "input_shape = (330, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3db221b7-8187-4c26-afa0-3d467ff78c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aysen\\anaconda3\\envs\\hate_speech_classification\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\aysen\\anaconda3\\envs\\hate_speech_classification\\Lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Char-Level CNN-LSTM Model\n",
    "char_cnn_lstm_model = Sequential([\n",
    "    \n",
    "    # Three convolution layers\n",
    "    Conv1D(256, kernel_size=3, strides=1, padding='valid', activation=LeakyReLU(alpha=0.1), input_shape=input_shape),\n",
    "    Conv1D(256, kernel_size=3, strides=1, padding='valid', activation=LeakyReLU(alpha=0.1)),\n",
    "    MaxPooling1D(pool_size=3),\n",
    "    Conv1D(256, kernel_size=3, strides=1, padding='valid', activation=LeakyReLU(alpha=0.1)),\n",
    "    MaxPooling1D(pool_size=3),\n",
    "\n",
    "    # LSTM layer\n",
    "    LSTM(100, dropout=0.5, recurrent_dropout=0.5, return_sequences=True),\n",
    "\n",
    "    # Max-pooling layer\n",
    "    MaxPooling1D(pool_size=3),\n",
    "    # GlobalMaxPooling1D()\n",
    "    \n",
    "    # Flatten layer\n",
    "    Flatten(),\n",
    "\n",
    "    # Output layer\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5d2b599-17f4-4b45-97d4-37eed6d3b6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 328, 256)          21760     \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 326, 256)          196864    \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 108, 256)          0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 106, 256)          196864    \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 35, 256)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 35, 100)           142800    \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPoolin  (None, 11, 100)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1100)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 1101      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 559389 (2.13 MB)\n",
      "Trainable params: 559389 (2.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "char_cnn_lstm_model.compile(loss='binary_crossentropy',  optimizer=Adam(learning_rate=learning_rate), metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "char_cnn_lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86a7a76c-1db5-4a1e-8c56-5511df899bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From C:\\Users\\aysen\\anaconda3\\envs\\hate_speech_classification\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\aysen\\anaconda3\\envs\\hate_speech_classification\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "48/48 [==============================] - 9s 95ms/step - loss: 0.6632 - accuracy: 0.6016 - val_loss: 0.6372 - val_accuracy: 0.6580\n",
      "Epoch 2/20\n",
      "48/48 [==============================] - 4s 82ms/step - loss: 0.6489 - accuracy: 0.6185 - val_loss: 0.6276 - val_accuracy: 0.6762\n",
      "Epoch 3/20\n",
      "48/48 [==============================] - 4s 78ms/step - loss: 0.6328 - accuracy: 0.6460 - val_loss: 0.6134 - val_accuracy: 0.6319\n",
      "Epoch 4/20\n",
      "48/48 [==============================] - 4s 82ms/step - loss: 0.6246 - accuracy: 0.6349 - val_loss: 0.6263 - val_accuracy: 0.6658\n",
      "Epoch 5/20\n",
      "48/48 [==============================] - 4s 77ms/step - loss: 0.6000 - accuracy: 0.6708 - val_loss: 0.5821 - val_accuracy: 0.6658\n",
      "Epoch 6/20\n",
      "48/48 [==============================] - 4s 77ms/step - loss: 0.5450 - accuracy: 0.7315 - val_loss: 0.5474 - val_accuracy: 0.7206\n",
      "Epoch 7/20\n",
      "48/48 [==============================] - 4s 78ms/step - loss: 0.4778 - accuracy: 0.7734 - val_loss: 0.5977 - val_accuracy: 0.6789\n",
      "Epoch 8/20\n",
      "48/48 [==============================] - 4s 81ms/step - loss: 0.4110 - accuracy: 0.8132 - val_loss: 0.6153 - val_accuracy: 0.7102\n",
      "Epoch 9/20\n",
      "48/48 [==============================] - 4s 78ms/step - loss: 0.3244 - accuracy: 0.8550 - val_loss: 0.9315 - val_accuracy: 0.6815\n",
      "Epoch 10/20\n",
      "48/48 [==============================] - 4s 77ms/step - loss: 0.2732 - accuracy: 0.8877 - val_loss: 0.7188 - val_accuracy: 0.6919\n",
      "Epoch 11/20\n",
      "48/48 [==============================] - 4s 78ms/step - loss: 0.1830 - accuracy: 0.9373 - val_loss: 0.8673 - val_accuracy: 0.6710\n",
      "Epoch 12/20\n",
      "48/48 [==============================] - 4s 83ms/step - loss: 0.1265 - accuracy: 0.9530 - val_loss: 1.0535 - val_accuracy: 0.6762\n",
      "Epoch 13/20\n",
      "48/48 [==============================] - 4s 86ms/step - loss: 0.0906 - accuracy: 0.9673 - val_loss: 1.0629 - val_accuracy: 0.6971\n",
      "Epoch 14/20\n",
      "48/48 [==============================] - 4s 77ms/step - loss: 0.0789 - accuracy: 0.9713 - val_loss: 1.2067 - val_accuracy: 0.6919\n",
      "Epoch 15/20\n",
      "48/48 [==============================] - 4s 77ms/step - loss: 0.0800 - accuracy: 0.9706 - val_loss: 1.0838 - val_accuracy: 0.6893\n",
      "Epoch 16/20\n",
      "48/48 [==============================] - 4s 77ms/step - loss: 0.0277 - accuracy: 0.9928 - val_loss: 1.3494 - val_accuracy: 0.6945\n"
     ]
    }
   ],
   "source": [
    "# Early stopping\n",
    "callback = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "\n",
    "# Train the model\n",
    "char_cnn_lstm_history = char_cnn_lstm_model.fit(X_train_tensor, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837474a0-300b-4441-af43-672f7427e89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve training results\n",
    "train_loss = char_cnn_lstm_history.history[\"loss\"]\n",
    "train_acc  = char_cnn_lstm_history.history[\"accuracy\"]\n",
    "valid_loss = char_cnn_lstm_history.history[\"val_loss\"]\n",
    "valid_acc  = char_cnn_lstm_history.history[\"val_accuracy\"]\n",
    "   \n",
    "plot_results([ train_loss, valid_loss ],        \n",
    "            ylabel=\"Loss\", \n",
    "            ylim = [0.0, 2.0],\n",
    "            metric_name=[\"Training Loss\", \"Validation Loss\"],\n",
    "            color=[\"g\", \"b\"], epochs=epochs);\n",
    " \n",
    "plot_results([ train_acc, valid_acc ], \n",
    "            ylabel=\"Accuracy\",\n",
    "            ylim = [0.0, 1.0],\n",
    "            metric_name=[\"Training Accuracy\", \"Validation Accuracy\"],\n",
    "            color=[\"g\", \"b\"], epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55af86b-31f0-4eea-9511-dbe02b3f7a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = char_cnn_lstm_model.evaluate(X_test_tensor, y_test)\n",
    "print(f\"Test accuracy: {test_acc*100:.3f}\")\n",
    "\n",
    "y_pred = char_cnn_lstm_model.predict(X_test_tensor)\n",
    "\n",
    "# Convert predictions to binary values (0 or 1) based on a threshold\n",
    "threshold = 0.5\n",
    "y_pred_binary = [1 if pred > threshold else 0 for pred in y_pred]\n",
    "\n",
    "# Evaluate the performance using metrics like accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "\n",
    "print('\\n')\n",
    "print(classification_report(y_test, y_pred_binary, target_names=['noHate', 'hate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bef31d-e129-4a20-b93e-82347690fc0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
